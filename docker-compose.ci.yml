---
name: ft_transcendence
services:
  
  cloudflared:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: cloudflare/cloudflared:latest
    container_name: cloudflared
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "cloudflared"
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}

  postgres:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: "postgres:16"
    container_name: postgres
    # command: -c config_file=/etc/postgresql/postgresql.conf
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # - ./src/grafana/config/postgresql.conf:/etc/postgresql/postgresql.conf
      - postgres_logs:/var/log/postgresql
    environment:
      PGDATA: /var/lib/postgresql/data
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "postgres"
    healthcheck:
      test:
        - CMD-SHELL
        - pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -p 5432
      interval: 30s
      timeout: 30s
      retries: 5
  
  postgres-exporter:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: bitnami/postgres-exporter:latest
    container_name: postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
      DISABLE_DEFAULT_METRICS: "true"
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_started
    ports:
    - "9187:9187"
  
  redis:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build: src/redis
    container_name: redis
    ports:
      - 6380:6379
    volumes:
      - redis_data:/data
    logging:
      driver: gelf
      options:
        gelf-address: "udp://${LOG_HOST}:12201"
        tag: "redis"
    healthcheck:
      test:
        - CMD
        - redis-cli
        - ping
      interval: 10s
      timeout: 5s
      retries: 5
  
  backend:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build: src/ft_transcendence_backend
    container_name: backend
    environment:
      DJANGO_SECRET_KEY: ${DJANGO_SECRET_KEY}
      EMAIL_FROM: ${EMAIL_FROM}
      EMAIL_TO: ${EMAIL_TO}
      EMAIL_HOST: ${EMAIL_HOST}
      EMAIL_HOST_USER: ${EMAIL_HOST_USER}
      EMAIL_HOST_PASSWORD: ${EMAIL_HOST_PASSWORD}
      EMAIL_PORT: ${EMAIL_PORT}
      POSTGRES_USER: ${POSTGRES_USER}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD}
      POSTGRES_DB: ${POSTGRES_DB}
      DATABASE_URL: postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}
      CLOUDFLARE_TUNNEL_TOKEN: ${CLOUDFLARE_TUNNEL_TOKEN}
      ETH_PRIVATE_KEY: ${ETH_PRIVATE_KEY}
    depends_on:
      - postgres
      - redis
    ports:
      - 8000:8000
    volumes:
      - django_logs:/app/logs
    command: |
      sh -c "python manage.py makemigrations &&
             python manage.py migrate &&
             python manage.py runserver 0.0.0.0:8000"
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:8000/metrics || exit 1
      interval: 10s
      timeout: 10s
      retries: 5

  game:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build: src/ft_transcendence_backend/game
    container_name: game
    command: python3 -m uvicorn pong_game:app --host 0.0.0.0 --port 8001 --reload
    ports:
      - 8001:8001
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:8001 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5

  caddy:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build:
      context: ./src/
      dockerfile: caddy/Dockerfile
    container_name: caddy
    ports:
      - 8080:80
      - 8444:443
    depends_on:
      - backend
    extra_hosts:
      - host.docker.internal:host-gateway
    volumes:
      - caddy_logs:/var/log/caddy
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:80 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
  
  grafana:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.grafana
    container_name: grafana
    ports:
      - 3000:3000
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_BASIC_ENABLED=false
      - ENABLE_LOGS_GRAFANA=true
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=redis-datasource
    volumes:
      - grafana_data:/var/lib/grafana
      - grafana_logs:/var/log/grafana
    depends_on:
      - redis
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f http://localhost:3000 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5

 

  portainer:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build: src/portainer
    container_name: portainer
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - portainer-data:/data
    depends_on:
      - caddy
    ports:
      - 9443:9443
    command:
      - --metrics
      - --metrics-address=:9443
    healthcheck:
      test:
        - CMD-SHELL
        - curl -f -k https://localhost:9443 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5

  prometheus:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.prometheus
    container_name: prometheus
    ports:
      - 9090:9090
    volumes:
      - prometheus_data:/prometheus
    healthcheck:
      test:
        - CMD-SHELL
        - wget -q -O- http://localhost:9090/metrics >/dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5


  elasticsearch:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: elasticsearch:8.17.0
    container_name: elasticsearch
    volumes:
    - elastic_data:/usr/share/elasticsearch/data/
    environment:
      - ES_JAVA_OPTS=-Xmx256m -Xms256m
      - xpack.monitoring.collection.enabled=true
      - discovery.type=single-node
      - xpack.security.enabled=false  # Enable security    
      # - ELASTIC_PASSWORD=LYCGEtv92deGJROBL*lp  # Set Elasticsearch password    
    ports:
    - '9200:9200'
    - '9300:9300'
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:9200/_cluster/health?wait_for_status=yellow || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  logstash:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    container_name: logstash
    build: ./src/elk/logstash
    profiles: ["elkprofile"]
    command: logstash -f /logstash_dir/logstash.conf 
    # depends_on:
    #   - elasticsearch
    ports:
      - "5000:5000/udp"  # Expose UDP port 5000 for syslog
      - "9600:9600"
      - "12201:12201/udp"
    environment:
      - ES_JAVA_OPTS=-Xmx1g -Xms1g
      - xpack.monitoring.enabled=true
      - xpack.monitoring.elasticsearch.hosts=http://elasticsearch:9200
      # - xpack.monitoring.elasticsearch.username=elastic
      # - xpack.monitoring.elasticsearch.password=LYCGEtv92deGJROBL*lp    
    volumes:
      - caddy_logs:/var/log/caddy
      - django_logs:/app/logs
      - postgres_logs:/var/log/postgresql
      - redis_logs:/var/log/redis
      - grafana_logs:/var/log/grafana

  kibana:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    container_name: kibana
    image: kibana:8.17.0
    profiles: ["elkprofile"]
    ports:
    - '5601:5601'
    environment:
      - ELASTICSEARCH_URL=http://elasticsearch:9200  
      # - ELASTICSEARCH_USERNAME=kibana_system
      # - ELASTICSEARCH_PASSWORD=LYCGEtv92deGJROBL*lp  # Set Kibana system password
    #   - xpack.encryptedSavedObjects.encryptionKey=<5880da9738edab6101dc26330c2fafca>
    #   - xpack.reporting.encryptionKey=<e1921b1a405bc83d11084504c65d8cae>
    #   - xpack.security.encryptionKey=<789dcf21ca81e07d37c6aa2c23742021>
    # depends_on:
    #   elasticsearch:
    #     condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  
  node-exporter:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: prom/node-exporter:latest
    container_name: node-exporter
    ports:
      - 9101:9100
    healthcheck:
      test:
        - CMD-SHELL
        - wget -q -O- http://localhost:9100/metrics >/dev/null 2>&1 || exit 1
      interval: 30s
      timeout: 10s
      retries: 5
  
  promtail:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.promtail
    container_name: promtail
    volumes:
      - promtail_logs:/app/logs
      - postgres_data:/var/lib/postgresql/data
      - redis_logs:/var/log/redis
      - caddy_logs:/var/log/caddy
      - django_logs:/app/logs
      - grafana_logs:/var/log/grafana
    ports:
      - 9080:9080
    healthcheck:
      test:
        - CMD-SHELL
        - |-
          bash -c 'printf "GET / HTTP/1.1

          " > /dev/tcp/127.0.0.1/9080; exit $?;'
      interval: 30s
      timeout: 10s
      retries: 5

  alertmanager:
    env_file:
      - .env.ci
    restart: always
    networks:
      - u42network
    init: true
    tty: true
    stdin_open: true
    image: prom/alertmanager:latest
    container_name: alertmanager
    volumes:
      - ./src/grafana/config/alertmanager.yml:/etc/alertmanager/alertmanager.yml
    environment:
      - EMAIL_HOST=${EMAIL_HOST}
      - EMAIL_PORT=${EMAIL_PORT}
      - EMAIL_HOST_USER=${EMAIL_HOST_USER}
      - EMAIL_HOST_PASSWORD=${EMAIL_HOST_PASSWORD}
    ports:
      - "9093:9093"

volumes:
  caddy_logs: null
  postgres_data: null
  postgres_logs: null
  redis_data: null
  redis_logs: null
  django_logs: null
  grafana_data: null
  grafana_logs: null
  grafana_provisioning: null
  prometheus_data: null
  portainer-data: null
  elastic_data: null
  logstash-persistence: null
  kibana_data: null
  promtail_logs: null
  elasticsearch_data: null

networks:
  u42network:
    name: u42_network
    driver: bridge
