name: u42

# these are common to all containers
x-common: &common
  profiles:
    - gameprofile
    - grafanaprofile
    - elkprofile
  env_file:
    - .env
  restart: always
  init: true
  tty: true
  stdin_open: true

services:
  postgres:
    <<: *common
    image: "postgres:16"
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: postgres
    # command: -c config_file=/etc/postgresql/postgresql.conf
    volumes:
      - postgres_data:/var/lib/postgresql/data
      # - ./src/grafana/config/postgresql.conf:/etc/postgresql/postgresql.conf
      - postgres_logs:/var/log/postgresql
    environment:
      PGDATA: /var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER} -d ${POSTGRES_DB} -p 5432"]
      interval: 10s
      timeout: 5s
      retries: 5

  postgres-exporter:
    <<: *common
    image: bitnami/postgres-exporter:latest
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: postgres-exporter
    environment:
      DATA_SOURCE_NAME: "postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB}?sslmode=disable"
      DISABLE_DEFAULT_METRICS: "true"
    depends_on:
      postgres:
        condition: service_healthy
      prometheus:
        condition: service_started
    ports:
    - "9187:9187"

  redis:
    <<: *common
    build: src/redis
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: redis
    ports:
      - "6380:6379"
    volumes:
      - redis_data:/data
      - redis_logs:/var/log/redis
    healthcheck:
      test: ["CMD", "redis-cli", "ping"] 
      interval: 10s
      timeout: 5s
      retries: 5

  backend:
    <<: *common
    build: src/ft_transcendence_backend
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: backend
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    ports:
      - "8000:8000"
    volumes:
      - django_logs:/app/logs
    command: >
      sh -c "python manage.py makemigrations &&
             python manage.py migrate &&
             daphne -b 0.0.0.0 -p 8000 backend.asgi:application"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8000/metrics || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5

  game:
    <<: *common
    build: src/ft_transcendence_backend/game
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: game
    command: python3 -m uvicorn pong_game:app --host 0.0.0.0 --port 8001 --reload
    ports:
      - "8001:8001"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8001 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  caddy:
    <<: *common
    build:
      context: ./src/
      dockerfile: caddy/Dockerfile
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: caddy
    ports:
      - "8080:80"  
      - "8444:443"  
    depends_on:
      backend:
        condition: service_healthy
    extra_hosts:
      - "host.docker.internal:host-gateway"
    volumes:
      - caddy_logs:/var/log/caddy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:80 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  cloudflared:
    <<: *common
    image: cloudflare/cloudflared:latest
    profiles: ["gameprofile"]
    networks:
      - appnetwork
    container_name: cloudflared
    restart: unless-stopped
    command: tunnel --no-autoupdate run --token ${CLOUDFLARE_TUNNEL_TOKEN}
  
  grafana:
    <<: *common
    build: 
      context: ./src/grafana
      dockerfile: Dockerfile.grafana
    profiles: ["grafanaprofile"]
    networks:
      - appnetwork
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
      - GF_AUTH_BASIC_ENABLED=true
      - ENABLE_LOGS_GRAFANA=true
      - GF_SECURITY_ADMIN_USER=${GF_SECURITY_ADMIN_USER}
      - GF_SECURITY_ADMIN_PASSWORD=${GF_SECURITY_ADMIN_PASSWORD}
      - GF_INSTALL_PLUGINS=redis-datasource
      - GF_PLUGINS_ALLOW_LOADING_UNSIGNED_PLUGINS=redis-datasource
    depends_on:
      redis:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:3000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
     
  # portainer:
  #   <<: *common
  #   build: src/portainer
  #   container_name: portainer
  #   volumes:
  #     - "/var/run/docker.sock:/var/run/docker.sock:ro"
  #     - portainer-data:/data
  #   depends_on:
  #     - caddy
  #   ports:
  #     - "9443:9443"
  #   command: ["--metrics", "--metrics-address=:9443"]
  #   healthcheck:
  #     test: ["CMD-SHELL", "curl -f -k https://localhost:9443 || exit 1"]
  #     interval: 30s
  #     timeout: 10s
  #     retries: 5

  prometheus:
    <<: *common
    build: 
      context: ./src/grafana
      dockerfile: Dockerfile.prometheus
    profiles: ["grafanaprofile"]
    networks:
      - appnetwork
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - prometheus_data:/prometheus
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9090/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  node-exporter:
    <<: *common
    image: prom/node-exporter:latest
    container_name: node-exporter
    profiles: ["grafanaprofile"]
    networks:
      - appnetwork
    ports:
      - "9101:9100"
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9100/-/healthy || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
  
  alertmanager:
    <<: *common
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.alertmanager
    profiles: ["grafanaprofile"]
    networks:
      - appnetwork
    container_name: alertmanager
    environment:
      - EMAIL_HOST=${EMAIL_HOST}
      - EMAIL_PORT=587
      - EMAIL_HOST_USER=${EMAIL_HOST_USER}
      - EMAIL_HOST_PASSWORD=${EMAIL_HOST_PASSWORD}
    ports:
      - "9093:9093"

  promtail:
    <<: *common
    build:
      context: ./src/grafana
      dockerfile: Dockerfile.promtail
    container_name: promtail
    profiles: ["elkprofile"]
    networks:
      - appnetwork
    volumes:
      - promtail_logs:/app/logs
      - postgres_logs:/var/log/postgresql
      - redis_logs:/var/log/redis
      - caddy_logs:/var/log/caddy
      - django_logs:/app/logs
      - grafana_logs:/var/log/grafana
    ports:
    - "9080:9080" 
    healthcheck:
      test: ["CMD-SHELL", "bash -c 'printf \"GET / HTTP/1.1\n\n\" > /dev/tcp/127.0.0.1/9080; exit $?;'"]
      interval: 30s
      timeout: 10s
      retries: 5

  elasticsearch:
    <<: *common
    container_name: elasticsearch
    image: bitnami/elasticsearch:8.15.0
    profiles: ["elkprofile"]
    networks:
      - appnetwork
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false # Disable security for development (enable in production)
      - "ES_JAVA_OPTS=-Xms1g -Xmx1g" # Adjust memory as needed
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:9200/_cluster/health?wait_for_status=yellow || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

  logstash:
    <<: *common
    container_name: logstash
    build: ./src/elk/logstash
    profiles: ["elkprofile"]
    networks:
      - appnetwork
    volumes:
      - ./logstash/pipeline:/usr/share/logstash/pipeline/
    ports:
      - "5044:5044" # Beats input
      - "9600:9600" # Logstash API
    depends_on:
      elasticsearch:
        condition: service_healthy

  kibana:
    <<: *common
    container_name: kibana
    build: ./src/elk/kibana
    profiles: ["elkprofile"]
    networks:
      - appnetwork
    environment:
      - ELASTICSEARCH_HOSTS = http://elasticsearch:9200
      - XPACK_SECURITY_ENABLED = false
    ports:
      - "5601:5601"
    depends_on:
      elasticsearch:
        condition: service_healthy
    healthcheck:
      test: ["CMD-SHELL", "curl -s -f http://localhost:5601/api/status || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3

volumes:
  caddy_logs: null
  postgres_data: null
  postgres_logs: null
  redis_data: null
  redis_logs: null
  django_logs: null
  grafana_data: null
  grafana_logs: null
  grafana_provisioning: null
  prometheus_data: null
  portainer-data: null
  elasticsearch_data: null
  kibana_data: null
  logstash_data: null
  promtail_logs: null

  # loki_logs: null
  # loki-data: null
  # tempo_data: null
  # tempo_logs: null

networks:
  appnetwork:
    name: appnetwork
    driver: bridge


  # tempo:
  #   <<: *common
  #   build: 
  #     context: ./src/grafana
  #     dockerfile: Dockerfile.tempo
  #   container_name: tempo
  #   command: [ "-config.file=/etc/tempo.yml" ]
  #   volumes:
  #     - tempo_data:/tmp/tempo
  #     - tempo_logs:/var/log/tempo
  #   ports:
  #     - "3200:3200"   # tempo
  #     - "4317:4317"   # otlp grpc
  #     - "4318:4318"   # otlp http
  #   healthcheck:
  #     test: ["CMD-SHELL", "wget -q -O- http://localhost:3200/metrics >/dev/null 2>&1 || exit 1"]
  #     interval: 60s
  #     timeout: 30s
  #     retries: 5

    # loki:
    # <<: *common
    # build: 
    #   context: ./src/grafana
    #   dockerfile: Dockerfile.loki
    # container_name: loki
    # ports:
    #   - "3100:3100"
    # command: ["-config.file=/etc/loki/local-config.yml"]
    # volumes:
    #   # - loki_config:/etc/loki/
    #   - loki_logs:/var/log/loki 
    # healthcheck:
    #   test: ["CMD-SHELL", "nc -z localhost 3100 || exit 1"]
    #   interval: 60s
    #   timeout: 660s
    #   retries: 5